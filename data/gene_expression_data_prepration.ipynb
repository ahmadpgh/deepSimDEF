{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene Expression Data Preprocessing<a id='top'></a>\n",
    "**Sections:**<br>\n",
    "[0) Description](#0)<br>\n",
    "[1) Importing Modules and Packages](#1)<br>\n",
    "[2) Configuration](#2)<br>\n",
    "[3) Loading Gene Ontology](#3)<br>\n",
    "[4) Loading Genes and Annotations](#4)<br>\n",
    "[5) Loading Gene Expression data](#5)<br>\n",
    "[6) Computing Absolute Pearson Correlation](#6)<br>\n",
    "[7) Saving the Results](#7)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description<a id='0'></a>\n",
    "\n",
    "**Aim:** This jupyter notebook results in Gene Expression data of species of interest (e.g., _human_ or *yeast*) with which the deepSimDEF networks would be trained and evaluatied.\n",
    "\n",
    "---\n",
    "**Output file format:** (space separated)<br>\n",
    "_gene1_ _gene2_ _coexpression_correlation_ <br>\n",
    "CD44 ARHGEF1 0.56<br>\n",
    "POLR2G CTDP1 0.21<br>\n",
    "OR5L2 SLC7A11 0.11<br>\n",
    "SCNM1 CEP120 0.99<br>\n",
    "... <br>\n",
    "\n",
    "---\n",
    "Files needed for this preprocessing are:\n",
    "\n",
    " * **Gene ontology:** ['go.obo' file](http://current.geneontology.org/ontology/go.obo)<br><br>\n",
    " \n",
    " * **Association files:** [gene association files ingested from GO Consortium members](http://current.geneontology.org/products/pages/downloads.html)\n",
    "  * **Human** - [Gene Association file (Homo sapiens)](http://geneontology.org/gene-associations/goa_human.gaf.gz)\n",
    "  * **Yeast** - [Gene Association file (Saccharomyces cerevisiae)](http://current.geneontology.org/annotations/sgd.gaf.gz)<br><br>\n",
    "  \n",
    " * **Gene Expression data:** <br>\n",
    "         \n",
    "     * **Human** - [GenEx (Homo sapiens)](https://multid.se/genex/) (normalization should be applied to the data)<br>\n",
    "         \n",
    "     * **Yeast** - [Eisen et al. (Saccharomyces cerevisiae)](http://www.i3s.unice.fr/~pasquier/web/userfiles/downloads/datasets/EisenYeastData_Measures.txt) <br>\n",
    "\n",
    "---\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import<a id='1'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import easydict\n",
    "import linecache\n",
    "import pprint\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration<a id='2'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = 'yeast' # species of interest to load of and save the resut for\n",
    "\n",
    "if species=='human':\n",
    "    association_file_name = 'goa_human.gaf.gz' # human\n",
    "    association_file_url = 'http://geneontology.org/gene-associations/goa_human.gaf.gz'\n",
    "    expression_file = 'WholeBlood.Gene.Filter6_20.TPM10_20.normalizedLimmaVoom.txt'\n",
    "elif species=='yeast':\n",
    "    association_file_name = 'sgd.gaf.gz' # yeast\n",
    "    association_file_url = 'http://current.geneontology.org/annotations/sgd.gaf.gz'\n",
    "    expression_file = 'EisenYeastData_Measures.txt'\n",
    "    expression_url = 'http://www.i3s.unice.fr/~pasquier/web/userfiles/downloads/datasets/EisenYeastData_Measures.txt'\n",
    "    \n",
    "args = easydict.EasyDict({\n",
    "    \"go_dir\": 'gene_ontology/raw/',     # directory to the Gene Ontology 'go.obo' file\n",
    "    \"association_file_dir\": 'species/{}/association_file/raw'.format(species), # directory to the human association file\n",
    "    \"gene_expression_raw_dir\": 'species/{}/gene_expression/raw'.format(species),          # directory to the raw gene expression data\n",
    "    \"result_gene_ontology_dir\": 'species/{}/gene_expression/processed'.format(species),   # directory in which the results would be saved\n",
    "    \"max_num_pairs\": -1,            # maximum number of pairs randomly chosen (-1 means all)\n",
    "    \"download_gene_ontology\": True,    # download the latest version of gene ontology into the specified directory above\n",
    "    \"download_association_file\": True, # download association file of the specieis of interest into the specified directory above\n",
    "    #\"threshold\": 0.8,                   # absolute pearson correlations below this cutoff point would be removed\n",
    "    \"seed\": 2021                         # seed to make sure the random negative samples are reproducable\n",
    "})\n",
    "    \n",
    "os.makedirs(args.result_gene_ontology_dir, exist_ok=True)  # create 'result_gene_ontology_dir' folder (if it does not exist already)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "subontology_map = {\"C\":\"CC\", \"P\":\"BP\", \"F\":\"MF\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### asserting raw data exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{args.gene_expression_raw_dir}/{expression_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if species=='yeast':\n",
    "    if os.path.exists(f\"{args.gene_expression_raw_dir}/{expression_file}\") is not True:\n",
    "        os.makedirs(args.gene_expression_raw_dir, exist_ok=True)  # create 'gene_expression_raw_dir' folder (if it does not exist already)\n",
    "        r = requests.get(expression_url, allow_redirects=True)\n",
    "        open('{}/{}'.format(args.gene_expression_raw_dir, expression_file), 'wb').write(r.content)\n",
    "elif species=='human':\n",
    "    assert os.path.exists(f\"{args.gene_expression_raw_dir}/{expression_file}\") is True, f\"\\nYou need to download the expression file first using the link and guideline provided above! \\nPut the {expression_file} file in '{args.gene_expression_raw_dir}/' directory.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gene Ontology<a id='3'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.download_gene_ontology:\n",
    "    os.makedirs(args.go_dir, exist_ok=True)  # create 'data_loc' folder (if it does not exist already)\n",
    "    print(\"Downloading the latest version of Gene Ontology into '{}'...\".format(args.go_dir))\n",
    "    url = 'http://current.geneontology.org/ontology/go.obo'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('{}/go.obo'.format(args.go_dir), 'wb').write(r.content)\n",
    "\n",
    "print(\"Gene Ontology {}\".format(linecache.getline('{}/go.obo'.format(args.go_dir), 2))) # Now: releases/2020-10-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reading Gene Ontology to extract Terms and their Descriptive Names\"\"\"\n",
    "with open(\"{}/go.obo\".format(args.go_dir)) as f:\n",
    "    content = f.readlines()\n",
    "content = \"\".join([x for x in content])\n",
    "content = content.split(\"[Typedef]\")[0].split(\"[Term]\")\n",
    "print(\"Information of the last GO term in the file:\\n~~~~~~~~~~~~~~~~~~~~~~~~~{}\".format(content[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Going through every GO term and extract information needed ('id', 'alt_id', 'namespace', and 'is_obsolete')\"\"\"\n",
    "go_term_dict = {}\n",
    "for c in content:\n",
    "    go_id = ''\n",
    "    for l in c.split(\"\\n\"):\n",
    "        # id\n",
    "        if \"id: GO:\" in l[0:len(\"id: GO:\")]:\n",
    "            go_id = l.split(\"id: \")[1]\n",
    "            go_term_dict[go_id] = {}\n",
    "        # alt_id\n",
    "        if \"alt_id:\" in l[0:len(\"alt_id\")+1]:\n",
    "            go_term_dict[go_id].setdefault(\"alt_id\", []).append(l.split(\"alt_id: \")[1])\n",
    "        # namespace\n",
    "        if \"namespace:\" in l[0:len(\"namespace\")+1]:\n",
    "            go_term_dict[go_id][\"namespace\"] = l.split(\"namespace: \")[1]\n",
    "        # is_obsolete\n",
    "        if \"is_obsolete:\" in l[0:len(\"is_obsolete\")+1]:\n",
    "            go_term_dict[go_id][\"is_obsolete\"] = l.split(\"is_obsolete: \")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"printing how the key:values are organized for every GO term\"\"\"\n",
    "for i in range(15):\n",
    "    print(list(go_term_dict)[i], end=\": \")\n",
    "    pp.pprint(go_term_dict[list(go_term_dict)[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"grouping GO terms based on the sub-ontologies they belong to\"\"\"\n",
    "subontology_go_term_dict = {}\n",
    "for go_id in go_term_dict:\n",
    "    if not go_term_dict[go_id].get('is_obsolete', False): # or => if 'is_obsolete' not in go_term_dict[go_id]:\n",
    "        subontology_go_term_dict.setdefault(go_term_dict[go_id]['namespace'].split('_')[1][0].upper(), []).append(go_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"including 'alt_id' into the sub-ontology's groups of GO terms\"\"\"\n",
    "for go_id in go_term_dict:\n",
    "    if go_term_dict[go_id].get('alt_id', False): # or => if 'alt_id' in go_term_dict[go_id]:\n",
    "        for alt_id in go_term_dict[go_id].get('alt_id'):\n",
    "            subontology_go_term_dict[go_term_dict[go_id]['namespace'].split('_')[1][0].upper()].append(alt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"printing how the key:values are organized for different sub-ontologies\"\"\"\n",
    "for subontology in subontology_go_term_dict:\n",
    "    print(\"{} ({}):: {} <= {} GO term (with 'alt_id') => {}\".format(\n",
    "        subontology, \n",
    "        subontology_map[subontology], \n",
    "        \" \".join(subontology_go_term_dict[subontology][:3]), \n",
    "        len(subontology_go_term_dict[subontology]), \n",
    "        \" \".join(subontology_go_term_dict[subontology][-3:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Genes and Annotations<a id='4'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.download_association_file:\n",
    "    os.makedirs(args.association_file_dir, exist_ok=True)  # create 'data_loc' folder (if it does not exist already)\n",
    "    print(\"Downloading the latest version of association file into '{}'...\".format(args.association_file_dir))\n",
    "    r = requests.get(association_file_url, allow_redirects=True)\n",
    "    open('{}/{}'.format(args.association_file_dir, association_file_name), 'wb').write(r.content)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"{}/{}\".format(args.association_file_dir, association_file_name), sep='\\t', comment=\"!\", skip_blank_lines=True, header=None, dtype=str)\n",
    "df = df.iloc[:,[1, 2, 3, 4, 6, 8]]\n",
    "if len(df[df[3].isnull()])==0:\n",
    "    df = df[~df[3].str.contains(\"NOT\")]\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "else:\n",
    "    df = df[df[3].isnull()]\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "df = df.drop(df.columns[2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if species == \"human\":\n",
    "    df = pd.read_csv(\"{}/{}\".format('species/human/association_file/raw', 'goa_human.gaf.gz'), \n",
    "                     sep='\\t', comment=\"!\", skip_blank_lines=True, header=None, dtype='unicode')\n",
    "\n",
    "    df = df.iloc[:,[1, 2, 3, 4, 6, 8]]\n",
    "    df = df[df[3].isnull()]\n",
    "    df = df.drop(df.columns[2], axis=1)\n",
    "    df\n",
    "if species == \"yeast\":\n",
    "    df = pd.read_csv(\"{}/{}\".format('species/human/association_file/raw', 'goa_human.gaf.gz'), \n",
    "                     sep='\\t', comment=\"!\", skip_blank_lines=True, header=None, dtype='unicode')\n",
    "\n",
    "    df = df.iloc[:,[1, 2, 3, 4, 6, 8]]\n",
    "    df = df[df[3].isnull()]\n",
    "    df = df.drop(df.columns[2], axis=1)\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keeping track of the gene ids and their mappings\"\"\"\n",
    "protein_gene_id_map = {}\n",
    "for gene_id, protein_id in zip(df[1], df[2]):\n",
    "    protein_gene_id_map[protein_id] = gene_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### removing 'ND' and 'IEA' annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df[6]!='ND') & (df[6]!='IEA')].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"protein dictionary to keep track of annotations for proteins (from each sub-ontology)\"\"\"\n",
    "proteins_dict = {}\n",
    "for index, row in df.iterrows():\n",
    "    gene = row[1]\n",
    "    go_term_id = row[4]\n",
    "    subontology = row[8]\n",
    "    if go_term_id in subontology_go_term_dict[subontology]:\n",
    "        proteins_dict.setdefault(gene, dict()).setdefault(subontology, set()).add(go_term_id)\n",
    "        \n",
    "\"\"\"printing how the key:values are organized for every gene/protein\"\"\"\n",
    "for i in range(5):\n",
    "    print(list(proteins_dict)[i], end=\": \")\n",
    "    pp.pprint(proteins_dict[list(proteins_dict)[i]])\n",
    "print(\"\\nTotal number of genes/proteins annotated:\", len(proteins_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking into account only fully annotated genes/proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keeping track of fully annotated genes/proteins\"\"\"\n",
    "fully_annotated_proteins_wo_iea = []\n",
    "for protein in proteins_dict:\n",
    "    if len(proteins_dict[protein]) == 3:\n",
    "        fully_annotated_proteins_wo_iea.append(protein)\n",
    "print(\"Out of {} proteins {} are (experimentally or manually) annotated by all three sub-ontologies.\".format(len(proteins_dict), len(fully_annotated_proteins_wo_iea)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gene Expression data<a id='5'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading species raw gene expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gene_expression = pd.read_csv(\"{}/{}\".format(args.gene_expression_raw_dir, expression_file), sep='\\t')\n",
    "df_gene_expression.iloc[:, 0] = [i.split(\".\")[0] for i in df_gene_expression.iloc[:,0]] # useful for human\n",
    "df_gene_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing the missing values if needed (using _fancyimpute_ package) \n",
    "https://github.com/iskandr/fancyimpute <br><br>\n",
    "<code>pip install fancyimpute</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if species == 'yeast':\n",
    "    from fancyimpute import KNN#, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "\n",
    "    XY_incomplete = df_gene_expression.to_numpy()[:, 1:]\n",
    "    XY_filled_knn = KNN(k=10).fit_transform(XY_incomplete)\n",
    "    XY_filled_knn = np.round(XY_filled_knn, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if species == 'yeast':\n",
    "    # replacing the incomplete dataframe with the imputed one\n",
    "    df_gene_expression.iloc[:, 1:] = XY_filled_knn\n",
    "df_gene_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if species == 'human':\n",
    "    import urllib.parse\n",
    "    import urllib.request\n",
    "\n",
    "    GENENAME_ids = {}\n",
    "\n",
    "    url = 'https://www.uniprot.org/uploadlists/'\n",
    "\n",
    "    params = {\n",
    "    'from': 'ENSEMBL_ID',\n",
    "    'to': 'ACC',\n",
    "    'format': 'tab',\n",
    "    'query': \" \".join([i.split(\".\")[0] for i in df_gene_expression.Name])\n",
    "    }\n",
    "\n",
    "    data = urllib.parse.urlencode(params)\n",
    "    data = data.encode('utf-8')\n",
    "    req = urllib.request.Request(url, data)\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        response = f.read()\n",
    "    #print(response.decode('utf-8'))\n",
    "    for i, mapping in enumerate(response.decode('utf-8').strip().split(\"\\n\")):\n",
    "        if i!=0: \n",
    "            id1, id2 = mapping.split(\"\\t\")\n",
    "            GENENAME_ids[id1] = id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if species == 'human':\n",
    "    df_gene_expression = df_gene_expression[df_gene_expression['Name'].isin(GENENAME_ids)] # for the nagation add ~\n",
    "    df_gene_expression = df_gene_expression.reset_index(drop=True)\n",
    "    df_gene_expression.Name = [GENENAME_ids[i] for i in list(df_gene_expression.Name)]\n",
    "df_gene_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing proteins without complete annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gene_expression = df_gene_expression[df_gene_expression.iloc[:,0].isin(fully_annotated_proteins_wo_iea)] # for the nagation add ~\n",
    "df_gene_expression = df_gene_expression.reset_index(drop=True)\n",
    "df_gene_expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Absolute Pearson Correlation<a id='6'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpearson = np.abs(np.corrcoef(df_gene_expression.iloc[:, 1:].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list = {i:seq for i, seq in enumerate(df_gene_expression.iloc[:,0].to_numpy())}\n",
    "print(seq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Results<a id='7'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pairs_full = list(itertools.combinations(list(seq_list.keys()), 2))\n",
    "print(\"Full length of pairs:\", len(list_of_pairs_full))\n",
    "\n",
    "if args.max_num_pairs == -1:\n",
    "    list_of_pairs = [list_of_pairs_full[i] for i in np.random.choice(len(list_of_pairs_full), len(list_of_pairs_full), replace=False)]\n",
    "    args.max_num_pairs = len(list_of_pairs)\n",
    "else:\n",
    "    list_of_pairs = [list_of_pairs_full[i] for i in np.random.choice(len(list_of_pairs_full), len(list_of_pairs_full), replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f'{species}_gene_expression.tsv'\n",
    "print(f\"Saving data into the file '{file}' with the binning strategy.\")\n",
    "with open(f\"{args.result_gene_ontology_dir}/{file}\", 'w') as fw:\n",
    "    fw.write(\"Gene_1\\tGene_2\\tExpression_Value\\n\")\n",
    "    #while len(list_of_pairs)<args.max_num_pairs:\n",
    "    k = 0\n",
    "    for pair in list_of_pairs:\n",
    "        r = rpearson[pair[0], pair[1]]\n",
    "        if 0.8<=r:# and k<=args.max_num_pairs:\n",
    "            k = k + 1\n",
    "            # transformation of the correlation coefficient into a Fishersâ€™ Z-score will be done by arctanh (inverse hyperbolic tangent function)\n",
    "            fw.write(\"{}\\t{}\\t{}\\n\".format(seq_list[pair[0]], seq_list[pair[1]], np.round(np.arctanh(r), 8))) # The Fisher transform equals the inverse hyperbolic tangen/arctanh\n",
    "            #fw.write(\"{} {} {}\\n\".format(seq_list[pair[0]], seq_list[pair[1]], np.round(r, 8))) # The Fisher transform equals the inverse hyperbolic tangen/arctanh\n",
    "    k2 = 0\n",
    "    for pair in list_of_pairs:\n",
    "        r = rpearson[pair[0], pair[1]]\n",
    "        if 0.6<=r and r<0.8 and k2<k:# and k<=args.max_num_pairs:\n",
    "            k2 = k2 + 1\n",
    "            fw.write(\"{}\\t{}\\t{}\\n\".format(seq_list[pair[0]], seq_list[pair[1]], np.round(np.arctanh(r), 8))) # The Fisher transform equals the inverse hyperbolic tangen/arctanh\n",
    "    k2 = 0\n",
    "    for pair in list_of_pairs:\n",
    "        r = rpearson[pair[0], pair[1]]\n",
    "        if 0.4<=r and r<0.6 and k2<k:# and k<=args.max_num_pairs:\n",
    "            k2 = k2 + 1\n",
    "            fw.write(\"{}\\t{}\\t{}\\n\".format(seq_list[pair[0]], seq_list[pair[1]], np.round(np.arctanh(r), 8))) # The Fisher transform equals the inverse hyperbolic tangen/arctanh\n",
    "    k2 = 0\n",
    "    for pair in list_of_pairs:\n",
    "        r = rpearson[pair[0], pair[1]]\n",
    "        if 0.2<=r and r<0.4 and k2<k:# and k<=args.max_num_pairs:\n",
    "            k2 = k2 + 1\n",
    "            fw.write(\"{}\\t{}\\t{}\\n\".format(seq_list[pair[0]], seq_list[pair[1]], np.round(np.arctanh(r), 8))) # The Fisher transform equals the inverse hyperbolic tangen/arctanh\n",
    "    k2 = 0\n",
    "    for pair in list_of_pairs:\n",
    "        r = rpearson[pair[0], pair[1]]\n",
    "        if 0.0<=r and r<0.2 and k2<k:# and k<=args.max_num_pairs:\n",
    "            k2 = k2 + 1\n",
    "            fw.write(\"{}\\t{}\\t{}\\n\".format(seq_list[pair[0]], seq_list[pair[1]], np.round(np.arctanh(r), 8))) # The Fisher transform equals the inverse hyperbolic tangen/arctanh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"species/{species}/gene_expression/processed/{species}_gene_expression.tsv\", sep=\"\\t\", dtype=str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_genes = set(list(df.Gene_1) + list(df.Gene_2))\n",
    "print(f\"Number of {species} genes:\", len(ge_genes))\n",
    "with open(f'{args.result_gene_ontology_dir}/{species}_gene_expression_genes.tsv', 'w') as fw:\n",
    "    for gene in sorted(ge_genes):\n",
    "        fw.write(f\"{gene}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepSimDEF_env]",
   "language": "python",
   "name": "conda-env-deepSimDEF_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
