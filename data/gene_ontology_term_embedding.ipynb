{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition-based Gene Ontology Term Embedding<a id='top'></a>\n",
    "**Sections:**<br>\n",
    "[0) Description](#0)<br>\n",
    "[1) Importing Modules and Packages](#1)<br>\n",
    "[2) Configuration](#2)<br>\n",
    "[3) GO Term Definitions](#3)<br>\n",
    "[4) MEDLINE Bigrams](#4)<br>\n",
    "[5) GO term Embeddings Computation](#5)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description<a id='0'></a>\n",
    "\n",
    "**Aim:** This jupyter notebook results in Gene Ontology term embeddings with the help of their textual definitions.<br><br>\n",
    "The stopwords file and bigramwords of PubMed abstracts are already provided witht this project. Feel Free to replace them if your preference of other stopwords of bigramwords.<br>\n",
    "\n",
    "_* Requirement:_ To make this work, you need to make sure you have access to large amount of memory (despite working with sparse matrices in the code).<br>\n",
    "\n",
    "\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing moduls<a id='1'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import linecache\n",
    "\n",
    "from scipy.sparse import csr_matrix, lil_matrix, coo_matrix, dok_matrix, vstack\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import operator\n",
    "import easydict\n",
    "import pprint\n",
    "import gzip\n",
    "\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "tz = timezone('US/Eastern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration<a id='2'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"go_dir\": 'gene_ontology/raw/',             # directory to the Gene Ontology 'go.obo' file\n",
    "    \"go_embedding_dir\": 'gene_ontology/embedding/',     # directory to the Gene Ontology embedding file\n",
    "    \"stopwords_dir\": 'pubmed/Stoplist.dat',     # stopwords to drop uninformative bigrams\n",
    "    \"bigramwords_dir\": 'pubmed/bigramWords.gz', # bigrams from PubMed abstracts\n",
    "    \"embedding_dims\": [50, 100, 150, 200, 300], # list of embedding sizes\n",
    "    \"download_gene_ontology\": True              # whether to download the latest release of Gene Ontology\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.stopwords_dir, \"r\") as fr:\n",
    "    stopwords = [x.split(\"\\\\b\")[1]+\"$\" for x in fr.readlines()]#[0:1]\n",
    "    pattern = re.compile(\"|\".join(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_columns(sparse_matrix):\n",
    "    unique_columns = np.unique(np.nonzero(sparse_matrix)[1])\n",
    "    return sparse_matrix[:, unique_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GO Term Definitions<a id='3'></a>\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### updating go.obo file ( default: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.download_gene_ontology:\n",
    "    os.makedirs(args.go_dir, exist_ok=True)  # create 'data_loc' folder (if it does not exist already)\n",
    "    print(\"Downloading the latest version of Gene Ontology into '{}'...\".format(args.go_dir))\n",
    "    url = 'http://current.geneontology.org/ontology/go.obo'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('{}/go.obo'.format(args.go_dir), 'wb').write(r.content)\n",
    "\n",
    "print(\"Gene Ontology {}\".format(linecache.getline('{}/go.obo'.format(args.go_dir), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reading go.obo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reading Gene Ontology to extract Terms and their Descriptive Names\"\"\"\n",
    "with open(\"{}/go.obo\".format(args.go_dir)) as f:\n",
    "    content = f.readlines()\n",
    "content = \"\".join([x for x in content])\n",
    "content = content.split(\"[Typedef]\")[0].split(\"[Term]\")\n",
    "print(content[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of unique attributes for one GO term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()\n",
    "for c in content:\n",
    "    for l in c.split(\"\\n\"):\n",
    "        if \": \" in l: s.add(l.split(\": \")[0])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_term_dict = {}\n",
    "for c in content:\n",
    "    go_id = ''\n",
    "    for l in c.split(\"\\n\"):\n",
    "        # id\n",
    "        if \"id: GO:\" in l[0:len(\"id: GO:\")]:\n",
    "            go_id = l.split(\"id: \")[1]\n",
    "            go_term_dict[go_id] = {}\n",
    "        # alt_id\n",
    "        if \"alt_id:\" in l[0:len(\"alt_id\")+1]:\n",
    "            go_term_dict[go_id].setdefault(\"alt_id\", []).append(l.split(\"alt_id: \")[1])\n",
    "        # name\n",
    "        if \"name:\" in l[0:len(\"name\")+1]:\n",
    "            go_term_dict[go_id][\"name\"] = l.split(\"name: \")[1]\n",
    "        # synonym\n",
    "        if \"synonym:\" in l[0:len(\"synonym\")+1]:\n",
    "            go_term_dict[go_id].setdefault(\"synonym\",[]).append(l.split(\"synonym: \")[1].split('\"')[1])\n",
    "        # namespace\n",
    "        if \"namespace:\" in l[0:len(\"namespace\")+1]:\n",
    "            go_term_dict[go_id][\"namespace\"] = l.split(\"namespace: \")[1]\n",
    "        # def\n",
    "        if \"def:\" in l[0:len(\"def\")+1]:\n",
    "            go_term_dict[go_id][\"def\"] = l.split(\"def: \")[1].split('\" [')[0][1:]\n",
    "        # is_a\n",
    "        if \"is_a:\" in l[0:len(\"is_a\")+1]:\n",
    "            go_term_dict[go_id].setdefault(\"is_a\", []).append(l.split(\"is_a: \")[1].split(\" !\")[0])\n",
    "        # relationship\n",
    "        if \"relationship:\" in l[0:len(\"relationship\")+1]:\n",
    "            # part_of\n",
    "            if \"relationship: part_of \" in l[0:len(\"relationship: part_of \")+1]:\n",
    "                go_term_dict[go_id].setdefault(\"part_of\", []).append(l.split(\"relationship: part_of \")[1].split(\" !\")[0])\n",
    "            # has_part\n",
    "            if \"relationship: has_part \" in l[0:len(\"relationship: has_part \")+1]:\n",
    "                go_term_dict[go_id].setdefault(\"has_part\", []).append(l.split(\"relationship: has_part \")[1].split(\" !\")[0])\n",
    "            # regulates\n",
    "            if \"relationship: regulates \" in l[0:len(\"relationship: regulates \")+1]:\n",
    "                go_term_dict[go_id].setdefault(\"regulates\", []).append(l.split(\"relationship: regulates \")[1].split(\" !\")[0])\n",
    "            if \"relationship: positively_regulates \" in l[0:len(\"relationship: positively_regulates \")+1]:\n",
    "                go_term_dict[go_id].setdefault(\"regulates\", []).append(l.split(\"relationship: positively_regulates \")[1].split(\" !\")[0])\n",
    "            if \"relationship: negatively_regulates \" in l[0:len(\"relationship: negatively_regulates \")+1]:\n",
    "                go_term_dict[go_id].setdefault(\"regulates\", []).append(l.split(\"relationship: negatively_regulates \")[1].split(\" !\")[0])\n",
    "            # happens_during\n",
    "            if \"relationship: happens_during \" in l[0:len(\"relationship: happens_during \")+1]:\n",
    "                go_term_dict[go_id].setdefault(\"happens_during\", []).append(l.split(\"relationship: happens_during \")[1].split(\" !\")[0])\n",
    "            # ends_during\n",
    "            if \"relationship: ends_during \" in l[0:len(\"relationship: ends_during \")+1]:\n",
    "                go_term_dict[go_id].setdefault(\"ends_during\", []).append(l.split(\"relationship: ends_during \")[1].split(\" !\")[0])\n",
    "            # occurs_in\n",
    "            if \"relationship: occurs_in \" in l[0:len(\"relationship: occurs_in \")+1]:\n",
    "                go_term_dict[go_id].setdefault(\"occurs_in\", []).append(l.split(\"relationship: occurs_in \")[1].split(\" !\")[0])\n",
    "        # is_obsolete\n",
    "        if \"is_obsolete:\" in l[0:len(\"is_obsolete\")+1]:\n",
    "            go_term_dict[go_id][\"is_obsolete\"] = l.split(\"is_obsolete: \")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding children (GO term X subsumes what other GO term(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for go_id in go_term_dict:\n",
    "    parents = go_term_dict[go_id].get('is_a', [])\n",
    "    for parent in parents:\n",
    "        go_term_dict[parent].setdefault(\"subsumes\", []).append(go_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for go_id in go_term_dict:\n",
    "    regulated_gos = go_term_dict[go_id].get('regulates', [])\n",
    "    for regulated_go in regulated_gos:\n",
    "        go_term_dict[regulated_go].setdefault(\"regulated_by\", []).append(go_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for go_id in go_term_dict:\n",
    "    parts = go_term_dict[go_id].get('has_part', [])\n",
    "    for part in parts:\n",
    "        if go_id not in go_term_dict[part].get(\"part_of\", []): \n",
    "            go_term_dict[part].setdefault(\"part_of\", []).append(go_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for go_id in go_term_dict:\n",
    "    parts = go_term_dict[go_id].get('part_of', [])\n",
    "    for part in parts:\n",
    "        if go_id not in go_term_dict[part].get(\"has_part\", []): \n",
    "            go_term_dict[part].setdefault(\"has_part\", []).append(go_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(go_term_dict['GO:0007127'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_extension = {}\n",
    "\n",
    "# direct definition\n",
    "for go_id in list(go_term_dict):\n",
    "    if not go_term_dict[go_id].get('is_obsolete', False):\n",
    "        namespace = go_term_dict[go_id]['namespace']\n",
    "        definition_extension.setdefault(namespace, {})\n",
    "        definition_extension[namespace].setdefault(go_id, []).append(go_term_dict[go_id]['def'])\n",
    "\n",
    "relationships = ['is_a', 'subsumes', 'part_of', 'has_part', 'regulates', 'regulated_by', 'happens_during', 'ends_during', 'occurs_in']\n",
    "for relationship in relationships:\n",
    "    for go_id in list(go_term_dict):\n",
    "        if not go_term_dict[go_id].get('is_obsolete', False):\n",
    "            namespace = go_term_dict[go_id]['namespace']\n",
    "            for related_go_term in go_term_dict[go_id].get(relationship, []):\n",
    "                definition_extension[namespace][go_id].append(go_term_dict[related_go_term]['def'])\n",
    "                #print(go_term_dict[related_go_term]['def'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(definition_extension['cellular_component']))\n",
    "print(definition_extension['cellular_component']['GO:0000015'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "definition_extension_info = {}\n",
    "definition_extension_terms = set()\n",
    "for sub_ontology in list(definition_extension):\n",
    "    definition_extension_info[sub_ontology] = {}\n",
    "    print(sub_ontology.upper())\n",
    "    for row_num, go_id in enumerate(definition_extension[sub_ontology]):\n",
    "        definition = \" \".join(definition_extension[sub_ontology][go_id])\n",
    "        definition = re.sub(r\"[|\\/\\\\\\'\\\"%^*\\[\\](){}_~,.:;@#?!&$=<>+\\-]+\\ *\", \" \", definition) # replacing punctuations\n",
    "        definition = re.sub(r\"\\d+th|\\d+nd|\\d+rd|\\d+s|\\d+d|\\d+h|\\d+m| . \", \" \", definition)\n",
    "        definition = definition.lower()\n",
    "        definition = [term for term in definition.split() if not pattern.match(term)]\n",
    "        definition = \" \".join(definition) # replacing multiple whitespace\n",
    "        definition = {term: definition.split().count(term) for term in set(definition.split())}\n",
    "        definition_extension_info[sub_ontology][go_id] = definition\n",
    "        definition_extension_terms.update(set(definition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEDLINE Bigrams<a id='4'></a>\n",
    "\n",
    "Link to PubMed dump: https://www.nlm.nih.gov/databases/download/data_distrib_main.html\n",
    "\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "term_index_dict = {}\n",
    "bigrams_dict = {}\n",
    "frequency_cutoff = 0\n",
    "start_time = datetime.datetime.now(tz)\n",
    "former_iteration_endpint = start_time\n",
    "print(\"Be patient, it may take some time ... (ETA: 20 minutes)\")\n",
    "print(\"Time started: {}\".format(start_time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "with gzip.open(args.bigramwords_dir, \"rt\") as fr:\n",
    "    bigrams = fr.readlines()\n",
    "    for bigram in bigrams:\n",
    "        bigram = bigram.split(\"|\")\n",
    "        frequency = int(bigram[0])\n",
    "        if frequency <= frequency_cutoff: continue\n",
    "        #if frequency <= frequency_cutoff or bigram[1]!=bigram[1].encode('ascii', 'ignore').decode(\"utf-8\"): continue\n",
    "        terms = bigram[1].split()\n",
    "        if terms[0] in definition_extension_terms or terms[1] in definition_extension_terms:\n",
    "            if not re.match(pattern, terms[0]) and not re.match(pattern, terms[1]):\n",
    "                if terms[0] not in term_index_dict:\n",
    "                    term_index_dict[terms[0]] = index\n",
    "                    index += 1\n",
    "                if terms[1] not in term_index_dict:\n",
    "                    term_index_dict[terms[1]] = index\n",
    "                    index += 1\n",
    "                bigrams_dict[\"{} {}\".format(terms[0], terms[1])] = frequency\n",
    "current_time = datetime.datetime.now(tz)\n",
    "time_elapsed = current_time - start_time\n",
    "print(\"Time current: {}\".format(current_time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "print(\"Time elapsed: {}\".format(str(time_elapsed).split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of bigrams:\", len(bigrams_dict))\n",
    "print(\"Number of unique terms:\", len(term_index_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now(tz)\n",
    "former_iteration_endpint = start_time\n",
    "print(\"Be patient, it may take some time ... (ETA: 6 minutes)\")\n",
    "print(\"Time started: {}\".format(start_time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "pubmed_bigram_matrix = lil_matrix((len(term_index_dict), len(term_index_dict)), dtype=np.float32)\n",
    "for bigram in bigrams_dict:\n",
    "    t1, t2 = bigram.split()\n",
    "    indx_t1, indx_t2 = term_index_dict[t1], term_index_dict[t2]\n",
    "    pubmed_bigram_matrix[indx_t1, indx_t2] = bigrams_dict[bigram]\n",
    "    \n",
    "pubmed_co_occurrence_matrix = pubmed_bigram_matrix + pubmed_bigram_matrix.transpose()\n",
    "del(pubmed_bigram_matrix)\n",
    "current_time = datetime.datetime.now(tz)\n",
    "time_elapsed = current_time - start_time\n",
    "print(\"Time current: {}\".format(current_time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "print(\"Time elapsed: {}\".format(str(time_elapsed).split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GO term Embeddings Computation<a id='5'></a>\n",
    "\n",
    "[back to top](#top)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(matrix, smooth_val=1):\n",
    "    if 0 < smooth_val: matrix = matrix.todense() + smooth_val\n",
    "    matrix = matrix / np.sum(matrix)\n",
    "    sc, sr = np.sum(matrix, 0), np.sum(matrix, 1)\n",
    "    return np.log10(matrix / (sr * sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"biological_process\"\"\"\n",
    "print(\"FOC computation to do...\")\n",
    "first_order_matrix = lil_matrix((len(definition_extension_info['biological_process']), len(term_index_dict)), dtype=np.float32)\n",
    "for row_num, go_id in enumerate(definition_extension_info['biological_process']):\n",
    "    definition = definition_extension_info['biological_process'][go_id]\n",
    "    #print(definition)\n",
    "    num_of_content_words = np.sum(list(definition.values()))  # used for normalization\n",
    "    for content_term in definition:\n",
    "        if content_term in term_index_dict:\n",
    "            first_order_matrix[row_num, term_index_dict[content_term]] = definition[content_term]/num_of_content_words\n",
    "print(\"SOC computation to do...\")\n",
    "second_order_matrix_BP = first_order_matrix*pubmed_co_occurrence_matrix\n",
    "print(\"second_order_matrix_BP shape:\", second_order_matrix_BP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cellular_component\"\"\"\n",
    "print(\"FOC computation to do...\")\n",
    "first_order_matrix = lil_matrix((len(definition_extension_info['cellular_component']), len(term_index_dict)), dtype=np.float32)\n",
    "for row_num, go_id in enumerate(definition_extension_info['cellular_component']):\n",
    "    definition = definition_extension_info['cellular_component'][go_id]\n",
    "    #print(definition)\n",
    "    num_of_content_words = np.sum(list(definition.values()))  # used for normalization\n",
    "    for content_term in definition:\n",
    "        if content_term in term_index_dict:\n",
    "            first_order_matrix[row_num, term_index_dict[content_term]] = definition[content_term]/num_of_content_words\n",
    "print(\"SOC computation to do...\")\n",
    "second_order_matrix_CC = first_order_matrix*pubmed_co_occurrence_matrix\n",
    "print(\"second_order_matrix_CC shape:\", second_order_matrix_CC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"molecular_function\"\"\"\n",
    "print(\"FOC computation to do...\")\n",
    "first_order_matrix = lil_matrix((len(definition_extension_info['molecular_function']), len(term_index_dict)), dtype=np.float32)\n",
    "for row_num, go_id in enumerate(definition_extension_info['molecular_function']):\n",
    "    definition = definition_extension_info['molecular_function'][go_id]\n",
    "    #print(definition)\n",
    "    num_of_content_words = np.sum(list(definition.values()))  # used for normalization\n",
    "    for content_term in definition:\n",
    "        if content_term in term_index_dict:\n",
    "            first_order_matrix[row_num, term_index_dict[content_term]] = definition[content_term]/num_of_content_words\n",
    "print(\"SOC computation to do...\")\n",
    "second_order_matrix_MF = first_order_matrix*pubmed_co_occurrence_matrix\n",
    "print(\"second_order_matrix_MF shape:\", second_order_matrix_MF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_matrix_full = vstack([second_order_matrix_BP, second_order_matrix_CC, second_order_matrix_MF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del(first_order_matrix, pubmed_co_occurrence_matrix,\n",
    "    second_order_matrix_BP, second_order_matrix_CC, second_order_matrix_MF)\n",
    "\n",
    "print(\"PMI to do... (Full SOC matrix size: {})\".format(second_order_matrix_full.shape))\n",
    "second_order_matrix_full = pmi(second_order_matrix_full)\n",
    "\n",
    "print(\"Positive PMI to do...\")\n",
    "second_order_matrix_full[second_order_matrix_full < 0.0] = 0\n",
    "\n",
    "print(\"Shrinking PMI to do (removing all-zero features)...\")\n",
    "column_index = [i for i, x in enumerate((np.sum(second_order_matrix_full, 0)!=0).tolist()[0]) if x]\n",
    "second_order_matrix_full = second_order_matrix_full[:, column_index]\n",
    "    \n",
    "for embedding_dim in args.embedding_dims:\n",
    "    \n",
    "    print(\"SVD to do... (dimension: {})\".format(embedding_dim))\n",
    "    U, S, Vt = svds(second_order_matrix_full, embedding_dim)\n",
    "    LSA1 = U*S\n",
    "    LSA_BP = LSA1[:len(definition_extension_info['biological_process']), :]\n",
    "    LSA_CC = LSA1[len(definition_extension_info['biological_process']):\n",
    "                 len(definition_extension_info['biological_process'])+len(definition_extension_info['cellular_component']),\n",
    "                        :]\n",
    "    LSA_MF = LSA1[len(definition_extension_info['biological_process'])+len(definition_extension_info['cellular_component']):,\n",
    "                        :]\n",
    "    for sub_ontology in ['biological_process', 'cellular_component', 'molecular_function']:\n",
    "        if sub_ontology=='biological_process': LSA = LSA_BP\n",
    "        if sub_ontology=='cellular_component': LSA = LSA_CC\n",
    "        if sub_ontology=='molecular_function': LSA = LSA_MF\n",
    "        os.makedirs(args.go_embedding_dir, exist_ok=True)\n",
    "        with open('{}/GO_{}_Embeddings_{}D.emb'.format(args.go_embedding_dir, \"\".join([s[0].upper() for s in sub_ontology.split(\"_\")]), embedding_dim), \"w\") as fw:\n",
    "            for go_term_it, embedding in zip(list(definition_extension[sub_ontology]), LSA):\n",
    "                fw.write(\"{} {}\\n\".format(go_term_it, \" \".join([str(i) for i in embedding])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepSimDEF_last]",
   "language": "python",
   "name": "conda-env-deepSimDEF_last-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
